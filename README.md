# Batch Gradient Descent (BGD) Unitary Optimisation ‚Äî Octave Implementation

This repository contains an **Octave implementation** of a **Batch Gradient Descent (BGD)** algorithm designed to approximate a *reference unitary matrix* `Uref` through numerical optimisation.  
The method uses finite-difference gradients to minimise the Frobenius norm squared difference between `Uref` and a trial unitary matrix `Utest = UC(P)`.

The code produces detailed convergence plots, percentile analyses, and saves both the numerical results and the best unitary matrix obtained during optimisation.

---

## üß† Overview of the Algorithm

The optimisation follows these main steps:

1. **Reference Unitary Generation**  
   A random complex matrix `A` is generated and decomposed using a QR factorisation:
   \[
   U_\text{ref} = Q \cdot \frac{\text{diag}(R)}{|\text{diag}(R)|}
   \]
   ensuring that `Uref` is unitary.

2. **Optimisation Loop (Gradient Descent)**  
   For each simulation:
   - Initialise a random real matrix `P`.
   - Map `P` to a unitary matrix `Utest = UC(P)` via QR decomposition.
   - Evaluate the cost function:
     \[
     C = ||U_\text{ref} - U_\text{test}||_F^2
     \]
   - Compute the gradient of the cost function using **finite differences**:
     \[
     \frac{\partial C}{\partial P_{ij}} = 
     \frac{C(P_{ij} + \varepsilon) - C(P_{ij} - \varepsilon)}{2 \varepsilon}
     \]
   - Update the matrix `P` via gradient descent:
     \[
     P = P - \alpha \, \nabla_P C
     \]

3. **Convergence Tracking**
   - The algorithm records the cost at each iteration.
   - Convergence is reached when the cost drops below `0.01`.
   - Cost histories, percentiles, and iteration counts are plotted for visual inspection.

4. **Outputs**
   - Average and percentile cost plots.
   - Iteration counts until convergence.
   - The best unitary matrix found (`best_U_overall`) and the corresponding cost.

---

## ‚öôÔ∏è Parameters and Settings

| Parameter | Symbol | Default | Description |
|------------|--------|----------|--------------|
| Learning rate | Œ± | `0.35` | Step size for gradient descent updates. Affects convergence speed and stability. |
| Epsilon | Œµ | `1e-6` | Perturbation step used in finite-difference gradient calculation. |
| Iterations | *num_iter* | `50` | Maximum number of gradient descent iterations per simulation. |
| Random seed | ‚Äî | `1111` | Fixed seed for reproducibility (`rng(1111)`). |
| Simulations | *num_simulations* | user-defined | Number of independent optimisation runs. |

---

### üßÆ Notes on Learning Rate (Œ±)

- The default `Œ± = 0.35` works well for most cases (`d = 2‚Äì5`).  
- **Reducing Œ±** (e.g. to `0.25`) can improve stability for higher-dimensional matrices.  
- **Increasing Œ±** may accelerate convergence but risks overshooting minima.  
- The learning rate is easy to change:
  ```matlab. python.. etc.
  learning_rate = 0.35;  % modify as needed

  ## External Code Attribution

This project uses the `UC.m` function authored by Christoph Spengler (University of Vienna, 2011)
for the composite parameterisation of unitary matrices.


